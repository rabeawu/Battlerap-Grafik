{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e65bb0b-3c76-4d70-b529-876a8ef43dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yt-dlp\n",
      "  Downloading yt_dlp-2025.9.26-py3-none-any.whl.metadata (175 kB)\n",
      "Downloading yt_dlp-2025.9.26-py3-none-any.whl (3.2 MB)\n",
      "   ---------------------------------------- 0.0/3.2 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 1.0/3.2 MB 7.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 2.4/3.2 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.2/3.2 MB 6.0 MB/s eta 0:00:00\n",
      "Installing collected packages: yt-dlp\n",
      "Successfully installed yt-dlp-2025.9.26\n"
     ]
    }
   ],
   "source": [
    "# Benötigte Pakete:\n",
    "# !pip install yt-dlp pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9e72e67-e7b8-44ab-a5f3-53d9ec1409e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from yt_dlp.utils import DownloadError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06103574-df45-448e-9b2b-ab0a36b4fbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 1128 entries, 0 to 1127\n",
      "Series name: upload_date\n",
      "Non-Null Count  Dtype  \n",
      "--------------  -----  \n",
      "0 non-null      float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 8.9 KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"battle_data.csv\")\n",
    "df[\"upload_date\"].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "724780db-a8b6-4c76-b5fd-7afdd721836f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 neue Videos in Playlist gefunden.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos abrufen: 0Video [00:00, ?Video/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 neue Videos in Playlist gefunden.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Videos abrufen: 0Video [00:00, ?Video/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fertig] Finale CSV geschrieben: battles_data.csv (insg. 1120 Videos)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\Rabea\\AppData\\Local\\Temp\\ipykernel_7340\\3187547126.py:116: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_final = pd.concat([df_existing, df_partial], ignore_index=True).drop_duplicates(subset='id')\n"
     ]
    }
   ],
   "source": [
    "# ---- helper: safe atomic write ----\n",
    "def safe_write_csv_atomic(df, target_path):\n",
    "    tmp = target_path + '.tmp'\n",
    "    df.to_csv(tmp, index=False)\n",
    "    os.replace(tmp, target_path)    # atomarer Austausch\n",
    "\n",
    "# ---- Playlist-ID-Abfrage (schnell) ----\n",
    "def fetch_playlist_ids(playlist_url):\n",
    "    ydl_opts = {'quiet': True, 'extract_flat': True, 'no_warnings': True}\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        info = ydl.extract_info(playlist_url, download=False)\n",
    "        return [{'id': v['id'], 'title': v.get('title','')} for v in info['entries']]\n",
    "\n",
    "# ---- Vollständige Metadaten für ein Video (mit Fehlerbehandlung) ----\n",
    "def fetch_data_full(video_id):\n",
    "    ydl_opts = {'quiet': True, 'extract_flat': False, 'no_warnings': True}\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        try:\n",
    "            info = ydl.extract_info(f\"https://www.youtube.com/watch?v={video_id}\", download=False)\n",
    "            return {\n",
    "                'id': info['id'],\n",
    "                'title': info.get('title',''),\n",
    "                'upload_date': info.get('upload_date',''),\n",
    "                'view_count': info.get('view_count', 0),\n",
    "                'like_count': info.get('like_count', 0),\n",
    "                'comment_count': info.get('comment_count', 0),\n",
    "                'url': f\"https://www.youtube.com/watch?v={info['id']}\",\n",
    "                'status': 'active'\n",
    "            }\n",
    "        except Exception as e:\n",
    "            # jede Art von Fehler → markieren, Abruf läuft weiter\n",
    "            print(f\"Warnung: {video_id} konnte nicht abgerufen werden: {e}\")\n",
    "            return {\n",
    "                'id': video_id,\n",
    "                'title': '',\n",
    "                'upload_date': '',\n",
    "                'view_count': 0,\n",
    "                'like_count': 0,\n",
    "                'comment_count': 0,\n",
    "                'url': f\"https://www.youtube.com/watch?v={video_id}\",\n",
    "                'status': 'deleted'\n",
    "            }\n",
    "\n",
    "# ---- Robuste Update-Funktion mit Checkpointing / Resume ----\n",
    "def update_csv_with_checkpoints(playlists, csv_file, checkpoint_every=20):\n",
    "    \"\"\"\n",
    "    - csv_file: finaler Dateiname, z.B. 'battles_data.csv'\n",
    "    - checkpoint_every: nach wie vielen abgefragten Videos die partial-Datei geschrieben wird\n",
    "    \"\"\"\n",
    "    partial_file = csv_file + '.partial'\n",
    "\n",
    "    # 1) vorhandene finale CSV laden (falls vorhanden)\n",
    "    if os.path.exists(csv_file):\n",
    "        df_existing = pd.read_csv(csv_file)\n",
    "    else:\n",
    "        df_existing = pd.DataFrame(columns=['id','title','upload_date','view_count','like_count','comment_count','url','status'])\n",
    "\n",
    "    # 2) vorhandene partial (Zwischenspeicherung) laden (falls vorhanden)\n",
    "    if os.path.exists(partial_file):\n",
    "        df_partial = pd.read_csv(partial_file)\n",
    "    else:\n",
    "        df_partial = pd.DataFrame(columns=['id','title','upload_date','view_count','like_count','comment_count','url','status'])\n",
    "\n",
    "    # 3) bereits bekannte IDs (final + partial) bilden\n",
    "    existing_ids = set(df_existing['id'].astype(str).tolist()) | set(df_partial['id'].astype(str).tolist())\n",
    "\n",
    "    # 4) Hauptschleife: durch Playlists, neue IDs ermitteln und nur die neuen komplett abfragen\n",
    "    for playlist_url in playlists:\n",
    "        playlist_videos = fetch_playlist_ids(playlist_url)\n",
    "        new_battles = [v for v in playlist_videos if str(v['id']) not in existing_ids]\n",
    "        print(f\"{len(new_battles)} neue Battles in Playlist gefunden.\")\n",
    "\n",
    "        # fetched_this_run sammelt die in diesem Lauf neu abgefragten Videos\n",
    "        fetched_this_run = []\n",
    "\n",
    "        for i, v in enumerate(tqdm(new_battles, desc=\"Videos abrufen\", unit=\"Video\")):\n",
    "            info = fetch_data_full(v['id'])\n",
    "            fetched_this_run.append(info)\n",
    "            existing_ids.add(str(v['id']))\n",
    "\n",
    "            # periodisch in partial speichern (so gehen Daten bei Abbruch nicht verloren)\n",
    "            if (i+1) % checkpoint_every == 0:\n",
    "                # kombiniere vorhandene partial + fetched_this_run und schreibe partial_file\n",
    "                if df_partial.empty:\n",
    "                    df_to_save = pd.DataFrame(fetched_this_run)\n",
    "                else:\n",
    "                    df_to_save = pd.concat([df_partial, pd.DataFrame(fetched_this_run)], ignore_index=True).drop_duplicates(subset='id')\n",
    "                # direkt überschreiben (kein atomic rename nötig für partial)\n",
    "                df_to_save.to_csv(partial_file, index=False)\n",
    "                print(f\"[Checkpoint] Zwischengespeichert nach {i+1} neuen Battles (partial gespeichert).\")\n",
    "\n",
    "        # am Ende der Playlist: die aktuellen gefetchten in df_partial aufnehmen\n",
    "        if fetched_this_run:\n",
    "            if df_partial.empty:\n",
    "                df_partial = pd.DataFrame(fetched_this_run)\n",
    "            else:\n",
    "                df_partial = pd.concat([df_partial, pd.DataFrame(fetched_this_run)], ignore_index=True).drop_duplicates(subset='id')\n",
    "\n",
    "    # 5) Fertig mit allen Playlists: finale Datei atomar schreiben (merge existing + partial)\n",
    "    if df_partial.empty and df_existing.empty:\n",
    "        print(\"Keine Daten gefunden. Keine Datei geschrieben.\")\n",
    "        return\n",
    "\n",
    "    if df_existing.empty:\n",
    "        df_final = df_partial\n",
    "    else:\n",
    "        df_final = pd.concat([df_existing, df_partial], ignore_index=True).drop_duplicates(subset='id')\n",
    "\n",
    "    # atomare Speicherung der finalen CSV\n",
    "    safe_write_csv_atomic(df_final, csv_file)\n",
    "    print(f\"[Fertig] Finale CSV geschrieben: {csv_file} (insg. {len(df_final)} Battles)\")\n",
    "\n",
    "    # 6) partial löschen, weil alles jetzt final ist\n",
    "    if os.path.exists(partial_file):\n",
    "        os.remove(partial_file)\n",
    "        print(\"Partial-Datei entfernt.\")\n",
    "\n",
    "# ---- Anwendung ----\n",
    "playlists = [\n",
    "    \"https://www.youtube.com/playlist?list=PLlYYj-TzOlFko09dyz1xsfLtvZdwxKPfK\",\n",
    "    \"https://www.youtube.com/playlist?list=PLXpSN5kwAmcgB3KMBUCHtLC-SGwdF2xMa\"\n",
    "    # ggf. weitere Playlists\n",
    "]\n",
    "\n",
    "csv_file = \"battles_data.csv\"\n",
    "update_csv_with_checkpoints(playlists, csv_file, checkpoint_every=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b97be7b-f4b4-408a-a988-33dee33d1e32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
